---
weight: 1
title: "Thoughts about life and philosophy"
date: 2023-01-10
lastmod: 2022-09-19 20:34:56
draft: true
description: "Thoughts about life and philosophy"
images: []

tags: ["thoughts",]
categories: ["thoughts"]

lightgallery: true
fontawesome: true

toc:
  enable: false
  auto: false
code:
  copy: true
  maxShownLines: 50
math:
  enable: false
  # ...
mapbox:
  # ...
share:
  enable: false
  # ...
comment:
  enable: false
---

# To write about

* Difficulty of changing ideas, implasticity of the brain

# My Life Philosophy

## Society (need to work through)

* Every institution needs to have a feedback mechanism for incentivising good work and making good decisions. Without it, the goal of optimizations is not clear (repharase)
* The current voting system is one of the major issues in current democracy. One vote per person without a good way to express preference inevitably leads to polarisation and [the tyranny of the majority](https://en.wikipedia.org/wiki/Tyranny_of_the_majority#Concurrent_majority). Preferential voting and localism are imperfect ways of dealing with this issue and only somewhat reduce its effect. A much better way of dealing with this is quadratic voting, with a good introduction written by [Vitalik Buterin](https://vitalik.ca/general/2019/12/07/quadratic.html). I am a proponent of introducing quadratic voting wherever possible to be able to express the level of agreement and finding the intermediate option which everybody is equally happy (or unhappy with).
* The key to making a successful society is to have a good goal alignment - the same problem that arises in AI safety. Ergo quadratic voting. This determines the [final goals](https://en.wikipedia.org/wiki/Instrumental_convergence#Instrumental_and_final_goals). This is something we still have not agreed on - what is it that we are trying to achieve with the society?
* Once there is a goal alignment, the key thing is to set up the incentives in such a way to make working towards the common goal. This system needs to have strong error correction mechanisms because we are inevitably going to find loopholes as we go.
* Need to read in more detail the [original paper](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2003531)
* The (main) final goal of the society should be preservation of life. There can be multiple final goals as multiple orthogonal directions can be optimized independently.
* For preservation of life, existance of an intelligent species like humans is paramount.
* The distinction between "is" and "should" questions. I think in modern society, we came to rely on science to answer all of our questions. However, science can only answer "is" questions. I.e. if we do this, then these are the consequences. But we rarely openly discuss "should" questions. I.e. what should we do with that information, what are our values and how do we live by them. These questions don't have a correct answer and they only depend on what you define to be the set of your final goals. This is where philosophy starts playing the key role.
* We as a society have to get away from the notion of certainties - that's just not how the world (quantum) works. We need to be comfortable with probabilities and think in the terms of degrees of belief - ergo Bayesian statistic. 
* All of the above makes me believe that to make progress in society, we need to heavily invest in education and in particular in better representation of several key areas: statistics and logical fallacies, philosophy and history of philosophical ideas, understanding of cause-effect, statistical significance and correlations vs causation

## Bayesian statistics

Life is a bet. There is nothing we can be certain of. Ever. Uncertainty is scary. We all like to know facts about the world. Things which are true and can never change. We think we know some things for certain. That Sun is going to come out tomorrow. That if we drop something, it will fall to the ground. That the ground is solid and air gaseous. 
These things we are certain of. But are we?


## AI safety

Thinking about AI safety and the alignment problem, one thing occured to me. The (current) AI is not at all the same as human-level intelligence. The big difference is that we are continuously thinking and can do internal reasoning through time without interacting with the world. The current AI is an input-output model. Meaning that for it to think, you need to provide it an input and then it generates an output (with a possible change to the internal state). However, it doesn't save the state between conversation and it is not continuously evolving as it is getting feedback from the world. Sure, there is a training procedure, but that's separate from the inference. It is not like there is a single model that is conversing with everyone online, remembering those conversations and doing internal contemplating about them. 
Maybe that is not enough to completely stop it, but I definitely see it affecting it's evolution. However, I guess there is nothing stopping people from creating a truly *living* AI that has periods of internal contemplation together with persistant memory across conversations.
